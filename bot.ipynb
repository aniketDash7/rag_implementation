{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\"\n",
    "OPENAI_API = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents : list[Document]):\n",
    "    textsplitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 500,\n",
    "        length_function = len,\n",
    "        add_start_index = True\n",
    "    )   \n",
    "    chunks = textsplitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_chroma(chunks:list[Document]):\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(openai_api_key=OPENAI_API),persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH,glob=\"*.txt\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 688 chunks\n",
      "Mrs. Anthon was fast unwinding her philosophy of life, in the sympathetic manner of Western Americans, that takes for granted a neighbour’s interest in one’s affairs and does not comprehend reticence. Wilbur was apparently interested. But Miss Anthon, who had practised the power of watching ever for her mother’s garrulous tongue, while she attended to other matters, interfered.\n",
      "\n",
      "“Mr. Erard will show us his den, mamma. Isn’t the apartment delightful and interesting? It’s an old swell’s house. _Louis seize_ complete, just as it was, without any change. Mr. Erard found it quite by accident, he says, one day when he was wandering about in this quarter among the convents. He came down a side lane that runs into the rue Vaugirard. Just as he was leaving it, his eye happened to fall upon that old cypress in the court. He prowled about and found this nest.”\n",
      "{'source': 'data\\\\book.txt', 'start_index': 5736}\n",
      "Saved 688 chunks to chroma\n"
     ]
    }
   ],
   "source": [
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] query_text\n",
      "ipykernel_launcher.py: error: the following arguments are required: query_text\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aniket\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"query_text\",type=str,help=\"The Query Text\")\n",
    "args = parser.parse_args()\n",
    "query_text = args.query_text\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=OPENAI_API)\n",
    "db = Chroma(persist_directory=CHROMA_PATH,embedding_function=embedding_function)\n",
    "\n",
    "results = db.similarity_search_with_relevance_scores(query_text,k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results\")\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc,_score in results])\n",
    "print(context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
